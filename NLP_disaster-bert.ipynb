{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5443aef",
   "metadata": {},
   "source": [
    "# Solovev_NLP(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4064e1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:22:51.273984Z",
     "iopub.status.busy": "2022-03-09T12:22:51.273042Z",
     "iopub.status.idle": "2022-03-09T12:22:51.296494Z",
     "shell.execute_reply": "2022-03-09T12:22:51.295772Z",
     "shell.execute_reply.started": "2022-03-09T12:22:51.273941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Embedding, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D,GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence \n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Embedding\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import (precision_score,recall_score,f1_score,classification_report,accuracy_score)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf5c92f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:13:46.024533Z",
     "iopub.status.busy": "2022-03-09T12:13:46.021912Z",
     "iopub.status.idle": "2022-03-09T12:13:46.139900Z",
     "shell.execute_reply": "2022-03-09T12:13:46.139143Z",
     "shell.execute_reply.started": "2022-03-09T12:13:46.024498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf007b1",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6bb68a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:13:46.141800Z",
     "iopub.status.busy": "2022-03-09T12:13:46.141121Z",
     "iopub.status.idle": "2022-03-09T12:13:47.964583Z",
     "shell.execute_reply": "2022-03-09T12:13:47.963517Z",
     "shell.execute_reply.started": "2022-03-09T12:13:46.141768Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "def remove_stopwords(text):\n",
    "    sw = stopwords.words('english')\n",
    "    words = text.split(' ')\n",
    "    filtered = [w for w in words if w not in sw]\n",
    "    return ' '.join([str(v) for v in filtered])\n",
    "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
    "df_test['text']  = df_test['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec334fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words\n",
    "def remove_stopwords(text):\n",
    "    sw = stopwords.words('english')\n",
    "    words = text.split(' ')\n",
    "    filtered = [w for w in words if w not in sw]\n",
    "    return ' '.join([str(v) for v in filtered])\n",
    "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
    "df_test['text']  = df_test['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b944b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punct\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)    \n",
    "    return text.translate(table)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_punct)\n",
    "df_test['text'] = df_test['text'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703ec78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(remove_emojis)\n",
    "df_test['text'] = df_test['text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7debee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b64a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renam location names\n",
    "df_train['location'].replace({'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\"FLorida\":'USA',\"Nigeria\":'Africa',\"Kenya\":'Africa',\"Everywhere\":'Worldwide',\"San Francisco\":'USA', \"Florida\":'USA',\"United Kingdom\":'UK',\"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\"NYC\":'USA',\n",
    "\"Seattle\":'USA', \"Earth\":'Worldwide', \"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA', \"Texas\":'USA',\"London, UK\":'UK',\"Atlanta, GA\":'USA', \"Mumbai\":\"India\"},inplace=True)\n",
    "df_test['location'].replace({'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\"FLorida\":'USA',\"Nigeria\":'Africa',\"Kenya\":'Africa',\"Everywhere\":'Worldwide',\"San Francisco\":'USA', \"Florida\":'USA',\"United Kingdom\":'UK',\"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\"NYC\":'USA',\n",
    "\"Seattle\":'USA', \"Earth\":'Worldwide', \"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA', \"Texas\":'USA',\"London, UK\":'UK',\"Atlanta, GA\":'USA', \"Mumbai\":\"India\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8313c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontraction(tweet):\n",
    "\n",
    "    tweet = re.sub(r\"won\\'t\", \" will not\", tweet)\n",
    "    tweet = re.sub(r\"won\\'t've\", \" will not have\", tweet)\n",
    "    tweet = re.sub(r\"can\\'t\", \" can not\", tweet)\n",
    "    tweet = re.sub(r\"don\\'t\", \" do not\", tweet)    \n",
    "    tweet = re.sub(r\"can\\'t've\", \" can not have\", tweet)\n",
    "    tweet = re.sub(r\"ma\\'am\", \" madam\", tweet)\n",
    "    tweet = re.sub(r\"let\\'s\", \" let us\", tweet)\n",
    "    tweet = re.sub(r\"ain\\'t\", \" am not\", tweet)\n",
    "    tweet = re.sub(r\"shan\\'t\", \" shall not\", tweet)\n",
    "    tweet = re.sub(r\"sha\\n't\", \" shall not\", tweet)\n",
    "    tweet = re.sub(r\"o\\'clock\", \" of the clock\", tweet)\n",
    "    tweet = re.sub(r\"y\\'all\", \" you all\", tweet)\n",
    "    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
    "    tweet = re.sub(r\"n\\'t've\", \" not have\", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n",
    "    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "    tweet = re.sub(r\"\\'d've\", \" would have\", tweet)\n",
    "    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "    tweet = re.sub(r\"\\'ll've\", \" will have\", tweet)\n",
    "    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n",
    "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "    return tweet \n",
    "df_train['text'] = df_train['text'].apply(lambda x : decontraction(x))\n",
    "df_test['text'] = df_test['text'].apply(lambda x : decontraction(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d9e68",
   "metadata": {},
   "source": [
    "# Model  \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e625ab42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:32:34.264948Z",
     "iopub.status.busy": "2022-03-09T12:32:34.264606Z",
     "iopub.status.idle": "2022-03-09T12:32:35.331611Z",
     "shell.execute_reply": "2022-03-09T12:32:35.330322Z",
     "shell.execute_reply.started": "2022-03-09T12:32:34.264911Z"
    }
   },
   "outputs": [],
   "source": [
    "from bert import bert_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35da431f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:32:42.687445Z",
     "iopub.status.busy": "2022-03-09T12:32:42.686745Z",
     "iopub.status.idle": "2022-03-09T12:32:50.149274Z",
     "shell.execute_reply": "2022-03-09T12:32:50.148309Z",
     "shell.execute_reply.started": "2022-03-09T12:32:42.687404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "852ad787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:32:50.152420Z",
     "iopub.status.busy": "2022-03-09T12:32:50.152069Z",
     "iopub.status.idle": "2022-03-09T12:32:50.195794Z",
     "shell.execute_reply": "2022-03-09T12:32:50.194690Z",
     "shell.execute_reply.started": "2022-03-09T12:32:50.152376Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "967ca37c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-09T12:21:07.206157Z",
     "iopub.status.idle": "2022-03-09T12:21:07.206942Z",
     "shell.execute_reply": "2022-03-09T12:21:07.206752Z",
     "shell.execute_reply.started": "2022-03-09T12:21:07.206729Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_Process_data(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e93b0540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T12:21:42.681825Z",
     "iopub.status.busy": "2022-03-09T12:21:42.681274Z",
     "iopub.status.idle": "2022-03-09T12:21:42.691594Z",
     "shell.execute_reply": "2022-03-09T12:21:42.690574Z",
     "shell.execute_reply.started": "2022-03-09T12:21:42.681790Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 180\n",
    "epochs = 10\n",
    "batch_size = 30\n",
    "lr = 0.000001\n",
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_id = Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_id = Input(shape=(max_len,), dtype=tf.int32, name = \"segment_id\")\n",
    "    \n",
    "    _, sequence_output = bert_layer([input_word_id, input_mask, segment_id])\n",
    "    output = sequence_output[:, 0, :]\n",
    "    dense1 = Dense(units=256,activation='relu')(output)\n",
    "    dense1 = Dropout(0.4)(dense1)\n",
    "    dense2 = Dense(units=128, activation='relu')(dense1)\n",
    "    dense3 = Dropout(0.4)(dense2)\n",
    "    out = Dense(1, activation='sigmoid')(dense3)\n",
    "    \n",
    "    model = Model(inputs=[input_word_id, input_mask, segment_id],outputs=out)\n",
    "    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22772bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = pre_Process_data(df_train.text.values, tokenizer, max_len=max_len)\n",
    "test_input = pre_Process_data(df_test.text.values, tokenizer, max_len=max_len)\n",
    "train_labels = df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a198b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=max_len)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "train_history = model.fit(train_input, train_labels,validation_split=0.2,epochs=epochs,callbacks=[checkpoint], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c7e2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)\n",
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c075821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
